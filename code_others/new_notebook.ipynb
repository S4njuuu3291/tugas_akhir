{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca752b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'IPython'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,hamming_loss,accuracy_score,make_scorer,f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import shap\n",
    "\n",
    "df = pd.read_csv(\"data//re_dataset.csv\",encoding='latin-1')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\n",
    "    \"HS_Individual\", \"HS_Group\", \"HS_Religion\",\n",
    "    \"HS_Race\", \"HS_Physical\", \"HS_Gender\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7faaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "alay_df = pd.read_csv(\"data//new_kamusalay.csv\")\n",
    "alay_dict = dict(zip(alay_df[\"alay\"], alay_df[\"normal\"]))\n",
    "\n",
    "def normalize_alay(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([alay_dict.get(w, w) for w in words])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    text = re.sub(r\"\\buser\\b\", \" \", text)         # hapus USER anonim\n",
    "    text = re.sub(r\"\\brt\\b\", \" \", text)           # hapus retweet marker\n",
    "    text = text.replace(\"\\n\", \" \")                # hapus newline\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)   # hapus URL\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)      # hapus angka/simbol\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()      # hapus spasi berlebih\n",
    "    text = normalize_alay(text)                   # normalisasi pakai kamus CSV\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"Tweet\"] = df[\"Tweet\"].apply(clean_text)\n",
    "df.to_csv(\"data/clean_dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['HS','Abusive','HS_Other','HS_Weak','HS_Moderate','HS_Strong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cbfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"num_labels\"] = df[label_cols].sum(axis=1)\n",
    "df[[\"Tweet\", \"num_labels\"] + label_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['num_labels']>1].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3680]['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93763106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "\n",
    "X = df['Tweet']\n",
    "y = df.drop(['Tweet','num_labels'], axis=1)\n",
    "sss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4572a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = y_train.sum()\n",
    "label_props = y_train.mean()\n",
    "\n",
    "print(\"\\nProporsi (persentase positif) per label:\")\n",
    "print(label_props)\n",
    "\n",
    "label_counts = y_test.sum()\n",
    "label_props = y_test.mean()\n",
    "\n",
    "print(\"\\nProporsi (persentase positif) per label:\")\n",
    "print(label_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),max_features=20000)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "model_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cv(clf, X, y, cv, model_name=\"model\"): \n",
    "    subset_acc_scores, hamming_scores, f1_micro_scores, f1_macro_scores = [], [], [], []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx] if hasattr(y, \"iloc\") else y[train_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        subset_acc_scores.append(accuracy_score(y_val, y_pred))\n",
    "        hamming_scores.append(hamming_loss(y_val, y_pred))\n",
    "        f1_micro_scores.append(f1_score(y_val, y_pred, average=\"micro\"))\n",
    "        f1_macro_scores.append(f1_score(y_val, y_pred, average=\"macro\"))\n",
    "\n",
    "    subset_accuracy = np.mean(subset_acc_scores)\n",
    "    hamming_losses = np.mean(hamming_scores)\n",
    "    f1_micro = np.mean(f1_micro_scores)\n",
    "    f1_macro = np.mean(f1_macro_scores)\n",
    "\n",
    "    results = {\n",
    "        \"subset_acc_scores\": subset_acc_scores,\n",
    "        \"hamming_scores\": hamming_scores,\n",
    "        \"f1_micro_scores\": f1_micro_scores,\n",
    "        \"f1_macro_scores\": f1_macro_scores,\n",
    "        \"subset_accuracy\": subset_accuracy,\n",
    "        \"hamming_loss\": hamming_losses,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"f1_macro\": f1_macro\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(\"Mean Subset Accuracy:\", subset_accuracy)\n",
    "    print(\"Mean Hamming Loss:\", hamming_losses)\n",
    "    print(\"Mean Micro F1:\", f1_micro)\n",
    "    print(\"Mean Macro F1:\", f1_macro)\n",
    "\n",
    "    return {model_name: results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    "clf1 = OneVsRestClassifier(log_reg)\n",
    "\n",
    "svm = LinearSVC(class_weight=\"balanced\", max_iter=2000)\n",
    "clf2 = OneVsRestClassifier(svm)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "clf3 = OneVsRestClassifier(nb)\n",
    "    \n",
    "model_scores = {}\n",
    "model_scores.update(evaluate_model_cv(clf1, X_train_tfidf, y_train, mskf, model_name=\"Log_Reg\"))\n",
    "model_scores.update(evaluate_model_cv(clf2, X_train_tfidf, y_train, mskf, model_name=\"SVM\"))\n",
    "model_scores.update(evaluate_model_cv(clf3, X_train_tfidf, y_train, mskf, model_name=\"NB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    model: {\n",
    "        \"Subset Accuracy\": scores[\"subset_accuracy\"],\n",
    "        \"Hamming Loss\": scores[\"hamming_loss\"],\n",
    "        \"F1 Micro\": scores[\"f1_micro\"],\n",
    "        \"F1 Macro\": scores[\"f1_macro\"]\n",
    "    }\n",
    "    for model, scores in model_scores.items()\n",
    "}).T\n",
    "results_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e08bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_ml(clf, X_train, X_test, y_train, y_test, use_vectorizer=False):\n",
    "    if isinstance(X_train, pd.Series):\n",
    "            X_train = X_train.to_frame()\n",
    "            X_test = X_test.to_frame()\n",
    "    if use_vectorizer:\n",
    "        transformer = ColumnTransformer([\n",
    "            ('vectorizer', vectorizer, 'Tweet')\n",
    "        ])\n",
    "        pipeline = Pipeline([\n",
    "            ('transformer', transformer),\n",
    "            ('model', clf)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('model', clf)\n",
    "        ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred,zero_division=0))\n",
    "    print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n",
    "    print(\"Subset Accuracy:\", accuracy_score(y_test, y_pred),\"\\n\")\n",
    "\n",
    "    return pipeline,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_ratio(actual,pred):\n",
    "    actual = np.array(actual)\n",
    "    pred = np.array(pred)\n",
    "    return (actual == pred).all(axis=1).mean()\n",
    "\n",
    "def hamming_loss_(actual,pred):\n",
    "    actual = np.array(actual)\n",
    "    pred = np.array(pred)\n",
    "    return (actual != pred).mean()\n",
    "    \n",
    "def micro_f1(actual,pred):\n",
    "    actual = np.array(actual)\n",
    "    pred = np.array(pred)\n",
    "    TP = np.sum((actual == 1) & (pred == 1))\n",
    "    FN = np.sum((actual == 1) & (pred == 0))\n",
    "    FP = np.sum((actual == 0) & (pred == 1))\n",
    "    \n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    return 2*precision*recall/(precision+recall)\n",
    "\n",
    "def macro_f1(actual, pred):\n",
    "    actual = np.array(actual).T # Transpose biar iterasi per label (bukan per sampel)\n",
    "    pred = np.array(pred).T\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        TP = np.sum((actual[i] == 1) & (pred[i] == 1))\n",
    "        FN = np.sum((actual[i] == 1) & (pred[i] == 0))\n",
    "        FP = np.sum((actual[i] == 0) & (pred[i] == 1))\n",
    "\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "        if (precision + recall) > 0:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9ef898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81      1113\n",
      "           1       0.86      0.88      0.87      1009\n",
      "           2       0.65      0.78      0.71       716\n",
      "           3       0.57      0.74      0.64       397\n",
      "           4       0.52      0.74      0.61       159\n",
      "           5       0.56      0.85      0.68       113\n",
      "           6       0.46      0.82      0.59        65\n",
      "           7       0.38      0.67      0.48        61\n",
      "           8       0.69      0.81      0.74       745\n",
      "           9       0.61      0.76      0.68       677\n",
      "          10       0.49      0.73      0.58       341\n",
      "          11       0.60      0.79      0.68        95\n",
      "\n",
      "   micro avg       0.68      0.80      0.73      5491\n",
      "   macro avg       0.60      0.78      0.67      5491\n",
      "weighted avg       0.69      0.80      0.74      5491\n",
      " samples avg       0.43      0.45      0.42      5491\n",
      "\n",
      "Hamming Loss: 0.10133510503669957\n",
      "Subset Accuracy: 0.5353075170842825 \n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81      1113\n",
      "           1       0.89      0.89      0.89      1009\n",
      "           2       0.66      0.74      0.70       716\n",
      "           3       0.58      0.67      0.62       397\n",
      "           4       0.59      0.65      0.61       159\n",
      "           5       0.67      0.73      0.69       113\n",
      "           6       0.63      0.66      0.65        65\n",
      "           7       0.50      0.52      0.51        61\n",
      "           8       0.72      0.77      0.74       745\n",
      "           9       0.62      0.70      0.66       677\n",
      "          10       0.49      0.61      0.55       341\n",
      "          11       0.73      0.76      0.75        95\n",
      "\n",
      "   micro avg       0.71      0.76      0.74      5491\n",
      "   macro avg       0.66      0.71      0.68      5491\n",
      "weighted avg       0.72      0.76      0.74      5491\n",
      " samples avg       0.44      0.43      0.42      5491\n",
      "\n",
      "Hamming Loss: 0.09567198177676538\n",
      "Subset Accuracy: 0.5592255125284739 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_logreg,y_pred_logreg = training_ml(clf1, X_train, X_test, y_train, y_test, use_vectorizer=True)\n",
    "pipeline_svm,y_pred_svm = training_ml(clf2, X_train, X_test, y_train, y_test, use_vectorizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "092de9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.09567198177676538),\n",
       " np.float64(0.5592255125284739),\n",
       " np.float64(0.6815527657754917),\n",
       " np.float64(0.7350622043104957))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_loss_(y_test,y_pred_svm),exact_match_ratio(y_test,y_pred_svm),macro_f1(y_test,y_pred_svm),micro_f1(y_test,y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "19293dda",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[195]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m pred = y_pred[idx]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Buat mapping label\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m actual_labels = \u001b[43m[\u001b[49m\u001b[43mlabel_cols\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m pred_labels = [label_cols[j] \u001b[38;5;28;01mfor\u001b[39;00m j, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pred) \u001b[38;5;28;01mif\u001b[39;00m val == \u001b[32m1\u001b[39m]\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[195]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      8\u001b[39m pred = y_pred[idx]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Buat mapping label\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m actual_labels = [\u001b[43mlabel_cols\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(actual) \u001b[38;5;28;01mif\u001b[39;00m val == \u001b[32m1\u001b[39m]\n\u001b[32m     12\u001b[39m pred_labels = [label_cols[j] \u001b[38;5;28;01mfor\u001b[39;00m j, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pred) \u001b[38;5;28;01mif\u001b[39;00m val == \u001b[32m1\u001b[39m]\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "n_samples = 5\n",
    "sample_idx = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "\n",
    "y_pred= y_pred_svm\n",
    "for i, idx in enumerate(sample_idx):\n",
    "    text = X_test.iloc[idx]\n",
    "    actual = y_test.iloc[idx].values\n",
    "    pred = y_pred[idx]\n",
    "\n",
    "    # Buat mapping label\n",
    "    actual_labels = [label_cols[j] for j, val in enumerate(actual) if val == 1]\n",
    "    pred_labels = [label_cols[j] for j, val in enumerate(pred) if val == 1]\n",
    "\n",
    "    print(f\"\\n=== Sample {i+1} ===\")\n",
    "    print(\"Tweet:\", text)\n",
    "    print(\"Actual Labels:\", actual_labels)\n",
    "    print(\"Predicted Labels:\", pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "08bca599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Tweet: Dasar bodoh banget, bikin malu bangsa\n",
      "Predicted Labels: ['HS_Individual', 'HS_Group']\n",
      "\n",
      "========================================\n",
      "Tweet: Saya cinta Indonesia\n",
      "Predicted Labels: ['None']\n",
      "\n",
      "========================================\n",
      "Tweet: Orang dari ras X memang menyebalkan\n",
      "Predicted Labels: ['HS_Individual']\n",
      "\n",
      "========================================\n",
      "Tweet: Jangan hina agama saya!\n",
      "Predicted Labels: ['None']\n",
      "\n",
      "========================================\n",
      "Tweet: Kamu itu kayak babi\n",
      "Predicted Labels: ['HS_Individual']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_new_tweet(texts, clf, label_cols):\n",
    "    X_new = pd.DataFrame({'Tweet': texts})\n",
    "    y_pred = clf.predict(X_new)\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        pred_labels = [label_cols[j] for j, val in enumerate(y_pred[i]) if val == 1]\n",
    "\n",
    "        print(\"=\"*40)\n",
    "        print(f\"Tweet: {text}\")\n",
    "        print(f\"Predicted Labels: {pred_labels if pred_labels else ['None']}\")\n",
    "        print()\n",
    "\n",
    "new_tweets = [\n",
    "    \"Dasar bodoh banget, bikin malu bangsa\",\n",
    "    \"Saya cinta Indonesia\",\n",
    "    \"Orang dari ras X memang menyebalkan\",\n",
    "    \"Jangan hina agama saya!\",\n",
    "    \"Kamu itu kayak babi\"\n",
    "]\n",
    "\n",
    "predict_new_tweet(new_tweets, pipeline_svm, label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2bb6a698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat:  Orang cacat itu beban masyarakat.\n",
      "\n",
      "Kalimat:  Orang cacat itu beban masyarakat.\n",
      "Fitur aktif (5): ['beban', 'cacat', 'itu', 'masyarakat', 'orang']\n",
      "Total fitur aktif = 5, total subset = 32\n",
      "Jumlah subset yang dipakai: 32\n",
      "\n",
      "Model memiliki 6 label.\n",
      "\n",
      "=== HS_Individual ===\n",
      "beban: -0.0015\n",
      "cacat: 0.0151\n",
      "itu: 0.0591\n",
      "masyarakat: -0.1258\n",
      "orang: 0.0563\n",
      "Baseline: 0.2819\n",
      "Prediksi asli HS_Individual: 0.2792\n",
      "\n",
      "=== HS_Group ===\n",
      "beban: 0.0043\n",
      "cacat: -0.0751\n",
      "itu: -0.0071\n",
      "masyarakat: 0.0310\n",
      "orang: 0.0230\n",
      "Baseline: 0.2166\n",
      "Prediksi asli HS_Group: 0.1897\n",
      "\n",
      "=== HS_Religion ===\n",
      "beban: -0.0099\n",
      "cacat: -0.0562\n",
      "itu: 0.0330\n",
      "masyarakat: 0.0296\n",
      "orang: 0.0194\n",
      "Baseline: 0.1182\n",
      "Prediksi asli HS_Religion: 0.1289\n",
      "\n",
      "=== HS_Race ===\n",
      "beban: -0.0016\n",
      "cacat: -0.0203\n",
      "itu: 0.0209\n",
      "masyarakat: 0.0091\n",
      "orang: 0.0224\n",
      "Baseline: 0.0592\n",
      "Prediksi asli HS_Race: 0.0885\n",
      "\n",
      "=== HS_Physical ===\n",
      "beban: -0.0073\n",
      "cacat: 0.5284\n",
      "itu: 0.0165\n",
      "masyarakat: -0.0528\n",
      "orang: 0.1134\n",
      "Baseline: 0.0714\n",
      "Prediksi asli HS_Physical: 0.6907\n",
      "\n",
      "=== HS_Gender ===\n",
      "beban: -0.0018\n",
      "cacat: -0.0248\n",
      "itu: 0.0273\n",
      "masyarakat: -0.0151\n",
      "orang: -0.0202\n",
      "Baseline: 0.0858\n",
      "Prediksi asli HS_Gender: 0.0504\n",
      "\n",
      "{'HS_Individual': {'base_value': np.float64(0.28189389301839696), 'shap_values': {'beban': np.float64(-0.0014839169980768938), 'cacat': np.float64(0.015052502042554816), 'itu': np.float64(0.059143196624638295), 'masyarakat': np.float64(-0.12583872444591865), 'orang': np.float64(0.056342568670606524)}}, 'HS_Group': {'base_value': np.float64(0.21664939038274522), 'shap_values': {'beban': np.float64(0.004317360640891527), 'cacat': np.float64(-0.07508227419466296), 'itu': np.float64(-0.007075001530203184), 'masyarakat': np.float64(0.031025083018186884), 'orang': np.float64(0.023021065267122894)}}, 'HS_Religion': {'base_value': np.float64(0.1182128668234127), 'shap_values': {'beban': np.float64(-0.009942844569945757), 'cacat': np.float64(-0.0561785363940597), 'itu': np.float64(0.033023185349699), 'masyarakat': np.float64(0.029606003172553914), 'orang': np.float64(0.019397308353105172)}}, 'HS_Race': {'base_value': np.float64(0.05924307511940005), 'shap_values': {'beban': np.float64(-0.0016037408295790194), 'cacat': np.float64(-0.02032106028280911), 'itu': np.float64(0.020901685326200736), 'masyarakat': np.float64(0.009056886872959429), 'orang': np.float64(0.022390283150574022)}}, 'HS_Physical': {'base_value': np.float64(0.07141749259126051), 'shap_values': {'beban': np.float64(-0.007250252020296069), 'cacat': np.float64(0.5283663610256857), 'itu': np.float64(0.016482185999000087), 'masyarakat': np.float64(-0.052841186486964065), 'orang': np.float64(0.11339501049206324)}}, 'HS_Gender': {'base_value': np.float64(0.08576822006256002), 'shap_values': {'beban': np.float64(-0.0017515498359030462), 'cacat': np.float64(-0.024844331359173313), 'itu': np.float64(0.0273186701539688), 'masyarakat': np.float64(-0.015092886091076854), 'orang': np.float64(-0.02019407568467775)}}}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import kernel_shap\n",
    "importlib.reload(kernel_shap)\n",
    "\n",
    "clf_model = pipeline_logreg.named_steps['model']\n",
    "kalimat = \"Orang cacat itu beban masyarakat.\"\n",
    "\n",
    "print(\"Kalimat: \",kalimat)\n",
    "print(kernel_shap.shap_kernel_instance(kalimat, vectorizer, clf_model,label_names=label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61ee32e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# import pandas as pd\n",
    "\n",
    "# def kernel_shap_explainer(clf, vectorizer, sentence, nsamples=300):\n",
    "#     import shap\n",
    "#     import numpy as np\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     X_instance = vectorizer.transform([sentence])\n",
    "#     background = np.zeros(X_instance.shape)\n",
    "\n",
    "#     def predict_fn(X):\n",
    "#         return clf.predict_proba(X)[:, 1]\n",
    "\n",
    "#     explainer = shap.KernelExplainer(predict_fn, background)\n",
    "#     shap_values = explainer.shap_values(X_instance, nsamples=nsamples)\n",
    "\n",
    "#     active_idx = X_instance.nonzero()[1]\n",
    "#     feature_names = vectorizer.get_feature_names_out()\n",
    "#     fitur_aktif = feature_names[active_idx]\n",
    "#     shap_active = shap_values[0][active_idx]\n",
    "\n",
    "#     shap.waterfall_plot(\n",
    "#         shap.Explanation(\n",
    "#             values=shap_active,\n",
    "#             base_values=explainer.expected_value,\n",
    "#             feature_names=fitur_aktif\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "# shap.initjs()\n",
    "# kernel_shap_explainer(clf1, vectorizer, \"Orang ini bodoh banget dan jahat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e535de99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: array([ 3,  4,  5,  6,  7, 10, 11, 12, 13, 16, 17, 18, 19]),\n",
       "  1: array([ 0,  1,  2,  6,  7,  8,  9, 12, 13, 14, 15, 18, 19]),\n",
       "  2: array([ 0,  1,  2,  3,  4,  5,  8,  9, 10, 11, 14, 15, 16, 17])},\n",
       " {0: array([ 0,  1,  2,  8,  9, 14, 15]),\n",
       "  1: array([ 3,  4,  5, 10, 11, 16, 17]),\n",
       "  2: array([ 6,  7, 12, 13, 18, 19])})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Data\n",
    "X = np.arange(20)\n",
    "y = np.array([0]*8 + [1]*6 + [2]*6)\n",
    "K = 3\n",
    "\n",
    "def StratKFold(X,y,K):\n",
    "    classes = np.unique(y)\n",
    "    all_class_idx = {}\n",
    "    \n",
    "    for i in classes:\n",
    "        class_idx = np.where(y==i)[0]\n",
    "        all_class_idx[i] = np.array_split(class_idx,K)\n",
    "\n",
    "    train_folds = {}\n",
    "    test_folds = {}\n",
    "    for i in range(K):\n",
    "        temp = []\n",
    "        for c in classes:\n",
    "            temp.append(all_class_idx[c][i])\n",
    "        test_folds[i] = np.concatenate(temp)\n",
    "        train_folds[i] = np.setdiff1d(np.arange(len(y)),test_folds[i])\n",
    "    return train_folds,test_folds\n",
    "\n",
    "StratKFold(X,y,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e399fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 7, 8, 9]), array([0, 5, 6]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(10).reshape(-1, 1)\n",
    "y = np.array([\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 0],#\n",
    "    [0, 1, 1],#\n",
    "    [1, 0, 1],#\n",
    "    [0, 0, 1],#\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],#\n",
    "    [0, 0, 1],#\n",
    "    [1, 1, 0]#\n",
    "])\n",
    "\n",
    "def multilabel_train_test_split(X, y, test_size=0.3, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_samples = len(y)\n",
    "    n_test = int(np.floor(n_samples * test_size))   \n",
    "    \n",
    "    # distribusi global label\n",
    "    label_sum = y.sum(axis=0)\n",
    "    target_test = np.round(label_sum * test_size).astype(int)\n",
    "    current_test = np.zeros_like(label_sum)\n",
    "    \n",
    "    # inisialisasi list untuk index test\n",
    "    test_indices = set()\n",
    "\n",
    "    # buat urutan acak dari seluruh index\n",
    "    all_indices = np.arange(n_samples)\n",
    "    rng.shuffle(all_indices)\n",
    "\n",
    "    for idx in all_indices:\n",
    "        # stop kalau test set sudah penuh\n",
    "        if len(test_indices) >= n_test:\n",
    "            break\n",
    "\n",
    "        # hitung seberapa “berguna” data ini untuk balancing\n",
    "        label_vector = y[idx]\n",
    "        score = 0\n",
    "        for j, label_value in enumerate(label_vector):\n",
    "            if label_value == 1 and current_test[j] < target_test[j]:\n",
    "                score += 1\n",
    "        \n",
    "        # semakin banyak label yg belum tercapai, semakin “berguna”\n",
    "        if score > 0:\n",
    "            test_indices.add(idx)\n",
    "            current_test += label_vector\n",
    "\n",
    "    # sisanya jadi train\n",
    "    train_indices = np.setdiff1d(np.arange(n_samples), list(test_indices))\n",
    "\n",
    "    return train_indices, np.array(list(test_indices))\n",
    "\n",
    "multilabel_train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4090347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Test idx : [5 6 0 4]\n",
      "Train idx: [1 2 3 7 8 9]\n",
      "Distribusi label test : [2 2 2]\n",
      "Distribusi label train: [4 3 3]\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Fold 2 ===\n",
      "Test idx : [7 3 2]\n",
      "Train idx: [0 1 4 5 6 8 9]\n",
      "Distribusi label test : [2 1 2]\n",
      "Distribusi label train: [4 4 3]\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Fold 3 ===\n",
      "Test idx : [9 1 8]\n",
      "Train idx: [0 2 3 4 5 6 7]\n",
      "Distribusi label test : [2 2 1]\n",
      "Distribusi label train: [4 3 4]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([5, 6, 0, 4]), array([7, 3, 2]), array([9, 1, 8])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MultilabelStratifiedKFold_manual(X, y, K=3, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_samples = len(y)\n",
    "    n_labels = y.shape[1]\n",
    "    \n",
    "    # Hitung distribusi global per label\n",
    "    label_sum = y.sum(axis=0)\n",
    "    target_per_fold = label_sum / K\n",
    "    \n",
    "    # Inisialisasi penghitung label di setiap fold\n",
    "    fold_label_counts = np.zeros((K, n_labels), dtype=int)\n",
    "    \n",
    "    # Simpan index per fold\n",
    "    folds = [[] for _ in range(K)]\n",
    "    \n",
    "    # Acak urutan data agar tidak bias\n",
    "    indices = np.arange(n_samples)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # Loop semua data\n",
    "    for idx in indices:\n",
    "        sample_labels = y[idx]\n",
    "        # Cari fold yang \"paling butuh\" data ini\n",
    "        fold_scores = []\n",
    "        for f in range(K):\n",
    "            score = 0\n",
    "            for j in range(n_labels):\n",
    "                if sample_labels[j] == 1 and fold_label_counts[f, j] < target_per_fold[j]:\n",
    "                    score += 1\n",
    "            fold_scores.append(score)\n",
    "        \n",
    "        # Pilih fold dengan score tertinggi (paling kekurangan label data ini)\n",
    "        best_fold = np.argmax(fold_scores)\n",
    "        folds[best_fold].append(idx)\n",
    "        fold_label_counts[best_fold] += sample_labels\n",
    "    \n",
    "    # Convert list of folds jadi array index\n",
    "    fold_indices = [np.array(f) for f in folds]\n",
    "    \n",
    "    # Cetak hasil\n",
    "    for i, test_idx in enumerate(fold_indices):\n",
    "        train_idx = np.setdiff1d(np.arange(n_samples), test_idx)\n",
    "        print(f\"\\n=== Fold {i+1} ===\")\n",
    "        print(\"Test idx :\", test_idx)\n",
    "        print(\"Train idx:\", train_idx)\n",
    "        print(\"Distribusi label test :\", y[test_idx].sum(axis=0))\n",
    "        print(\"Distribusi label train:\", y[train_idx].sum(axis=0))\n",
    "        print(\"-\"*50)\n",
    "    \n",
    "    return fold_indices\n",
    "\n",
    "MultilabelStratifiedKFold_manual(X, y, K=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
