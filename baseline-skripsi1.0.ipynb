{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13870537,"sourceType":"datasetVersion","datasetId":8833146}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import f1_score, hamming_loss,classification_report\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport re\nimport time\npd.set_option('display.max_colwidth', 120)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T07:31:14.683924Z","iopub.execute_input":"2025-11-28T07:31:14.684267Z","iopub.status.idle":"2025-11-28T07:31:14.690489Z","shell.execute_reply.started":"2025-11-28T07:31:14.684241Z","shell.execute_reply":"2025-11-28T07:31:14.689351Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"label_cols = [\n    \"HS_Individual\", \"HS_Group\", \"HS_Religion\",\n    \"HS_Race\", \"HS_Physical\", \"HS_Gender\"\n]\n\ndf = pd.read_csv(\"/kaggle/input/datasethatespeechmultilabel/re_dataset.csv\",encoding='latin-1')\ndf = df.drop(columns=['HS','Abusive','HS_Other','HS_Weak','HS_Moderate','HS_Strong'])\n\ndef normalize_alay(text):\n    words = text.split()\n    return \" \".join([alay_dict.get(w, w) for w in words])\n\ndef clean_text(text):\n    text = str(text).lower()\n    \n    text = re.sub(r\"\\buser\\b\", \" \", text)         # hapus USER anonim\n    text = re.sub(r\"\\brt\\b\", \" \", text)           # hapus retweet marker\n    text = text.replace(\"\\n\", \" \")                # hapus newline\n    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)   # hapus URL\n    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)      # hapus angka/simbol\n    text = re.sub(r\"\\s+\", \" \", text).strip()      # hapus spasi berlebih\n    text = normalize_alay(text)                   # normalisasi pakai kamus CSV\n    \n    return text\n\nalay_df = pd.read_csv(\"/kaggle/input/datasethatespeechmultilabel/new_kamusalay.csv\")\nalay_dict = dict(zip(alay_df[\"alay\"], alay_df[\"normal\"]))\n\ndf[\"Tweet\"] = df[\"Tweet\"].apply(clean_text)\nfor text in df['Tweet'].head(5):\n    print(text)\n    print(\"\")\n\ndf[\"num_labels\"] = df[label_cols].sum(axis=1)\ndf.head()\n\ndf.to_csv(\"clean_data.csv\",index=False)\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n\nX = df['Tweet']\ny = df.drop(['Tweet','num_labels'], axis=1)\nsss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_idx, test_idx in sss.split(X, y):\n    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\nlabel_props = y_train.mean()\n\nprint(\"\\nProporsi (persentase positif) data train per label:\")\nprint(label_props)\n\nlabel_props = y_test.mean()\n\nprint(\"\\nProporsi (persentase positif) data test per label:\")\nprint(label_props)\n\nimport numpy as np\n\ndef exact_match_ratio(actual, pred):\n    actual = np.array(actual)\n    pred   = np.array(pred)\n    return np.mean(np.all(actual == pred, axis=1))\n\ndef hamming_loss_(actual, pred):\n    actual = np.array(actual)\n    pred   = np.array(pred)\n    N, L   = actual.shape\n\n    diff = actual != pred\n    return diff.sum() / (N * L)\n\ndef micro_f1(actual, pred):\n    actual = np.array(actual)\n    pred   = np.array(pred)\n\n    TP = np.sum((actual == 1) & (pred == 1))\n    FP = np.sum((actual == 0) & (pred == 1))\n    FN = np.sum((actual == 1) & (pred == 0))\n\n    if TP == 0:\n        return 0.0\n\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n\n    if precision + recall == 0:\n        return 0.0\n\n    return 2 * precision * recall / (precision + recall)\n\ndef macro_f1(actual, pred):\n    actual = np.array(actual)\n    pred   = np.array(pred)\n\n    L = actual.shape[1]\n    f1_scores = []\n\n    for j in range(L):\n        y_true = actual[:, j]\n        y_pred = pred[:, j]\n\n        TP = np.sum((y_true == 1) & (y_pred == 1))\n        FP = np.sum((y_true == 0) & (y_pred == 1))\n        FN = np.sum((y_true == 1) & (y_pred == 0))\n\n        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n        recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n\n        if precision + recall > 0:\n            f1 = 2 * precision * recall / (precision + recall)\n        else:\n            f1 = 0.0\n\n        f1_scores.append(f1)\n\n    return np.mean(f1_scores)\n\ndef training_ml(clf, X_train, X_test, y_train, y_test, use_vectorizer=False):\n\n    if isinstance(X_train, pd.Series):\n        X_train = X_train.to_frame()\n        X_test  = X_test.to_frame()\n\n    if use_vectorizer:\n        # Pakai TF-IDF yang SUDAH fit, jangan fit ulang\n        X_train_vec = vectorizer.transform(X_train['Tweet'])\n        X_test_vec  = vectorizer.transform(X_test['Tweet'])\n    else:\n        X_train_vec = X_train.values\n        X_test_vec  = X_test.values\n\n    clf_name = clf.estimator.__class__.__name__\n    print(\"Model:\", clf_name)\n    print(\"Training dimulai...\")\n\n    start = time.time()\n    clf.fit(X_train_vec, y_train)\n    end = time.time()\n\n    print(\"Training selesai.\")\n    print(\"Durasi:\", round(end-start, 3), \"detik\")\n\n    y_pred = clf.predict(X_test_vec)\n\n    print(\"Classification Report:\\n\",\n          classification_report(y_test, y_pred, zero_division=0))\n    print(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\n    # print(\"Subset Accuracy:\", accuracy_score(y_test, y_pred), \"\\n\")\n\n    return clf, y_pred\n\nvectorizer = TfidfVectorizer(\n    ngram_range=(1,1),\n    max_features=20000\n)\nvectorizer.fit(X_train)\n\nX_train_vec = vectorizer.transform(X_train)\nX_test_vec  = vectorizer.transform(X_test)\n\nlog_reg = LogisticRegression(class_weight=\"balanced\", max_iter=1000,random_state=42)\nclf1 = OneVsRestClassifier(log_reg)\n\nsvm = LinearSVC(class_weight=\"balanced\", max_iter=2000,random_state=42)\nclf2 = OneVsRestClassifier(svm)\n\npipeline_logreg,y_pred_logreg = training_ml(clf1, X_train, X_test, y_train, y_test, use_vectorizer=True)\npipeline_svm,y_pred_svm = training_ml(clf2, X_train, X_test, y_train, y_test, use_vectorizer=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T07:32:08.010230Z","iopub.execute_input":"2025-11-28T07:32:08.010976Z","iopub.status.idle":"2025-11-28T07:32:13.092644Z","shell.execute_reply.started":"2025-11-28T07:32:08.010941Z","shell.execute_reply":"2025-11-28T07:32:13.091292Z"}},"outputs":[{"name":"stdout","text":"di saat semua cowok berusaha melacak perhatian gue kamu lantas remehkan perhatian yang gue kasih khusus ke kamu basic kamu cowok bego\n\nsiapa yang telat memberi tau kamu edan sarap gue bergaul dengan cigax jifla calis sama siapa itu licew juga\n\nkadang aku berpikir kenapa aku tetap percaya pada tuhan padahal aku selalu jatuh berkali kali kadang aku merasa tuhan itu meninggalkan aku sendirian ketika orang tuaku berencana berpisah ketika kakakku lebih memilih jadi kristen ketika aku anak ter\n\naku itu aku dan ku tau matamu sipit tapi dilihat dari mana itu aku\n\nkaum cebong kafir sudah kelihatan dongoknya dari awal tambah dungu lagi haha\n\n\nProporsi (persentase positif) data train per label:\nHS_Individual    0.271476\nHS_Group         0.150831\nHS_Religion      0.060180\nHS_Race          0.043000\nHS_Physical      0.024490\nHS_Gender        0.023256\ndtype: float64\n\nProporsi (persentase positif) data test per label:\nHS_Individual    0.271450\nHS_Group         0.150721\nHS_Religion      0.060364\nHS_Race          0.042901\nHS_Physical      0.024677\nHS_Gender        0.023159\ndtype: float64\nModel: LogisticRegression\nTraining dimulai...\nTraining selesai.\nDurasi: 2.676 detik\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.64      0.76      0.70       715\n           1       0.53      0.70      0.61       397\n           2       0.52      0.79      0.62       159\n           3       0.55      0.89      0.68       113\n           4       0.46      0.80      0.58        65\n           5       0.45      0.69      0.55        61\n\n   micro avg       0.57      0.76      0.65      1510\n   macro avg       0.53      0.77      0.62      1510\nweighted avg       0.58      0.76      0.65      1510\n samples avg       0.29      0.32      0.29      1510\n\nHamming Loss: 0.07751202227284232\nModel: LinearSVC\nTraining dimulai...\nTraining selesai.\nDurasi: 0.649 detik\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.67      0.74      0.70       715\n           1       0.55      0.62      0.58       397\n           2       0.58      0.65      0.61       159\n           3       0.63      0.77      0.69       113\n           4       0.57      0.68      0.62        65\n           5       0.48      0.49      0.48        61\n\n   micro avg       0.61      0.69      0.65      1510\n   macro avg       0.58      0.66      0.61      1510\nweighted avg       0.61      0.69      0.65      1510\n samples avg       0.29      0.29      0.28      1510\n\nHamming Loss: 0.07143761073146039\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# import optuna\n\n# def objective_logistic(trial):\n#     # Hyperparameter space\n#     C = trial.suggest_float(\"C\", 1e-4, 10, log=True)\n#     penalty = trial.suggest_categorical(\"penalty\", [\"l2\"])\n#     solver = trial.suggest_categorical(\"solver\", [\"lbfgs\", \"liblinear\"])\n    \n#     clf = OneVsRestClassifier(\n#         LogisticRegression(\n#             C=C,\n#             penalty=penalty,\n#             solver=solver,\n#             class_weight=\"balanced\",\n#             max_iter=2000\n#         )\n#     )\n\n#     clf.fit(X_train_vec, y_train)\n#     pred = clf.predict(X_test_vec)\n\n#     score = micro_f1(y_test, pred)\n#     return score\n\n# study_lr = optuna.create_study(direction=\"maximize\")\n# study_lr.optimize(objective_logistic, n_trials=500)\n\n# print(\"Best LR Params:\", study_lr.best_params)\n# print(\"Best LR Score:\", study_lr.best_value)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T07:31:36.043327Z","iopub.execute_input":"2025-11-28T07:31:36.043700Z","iopub.status.idle":"2025-11-28T07:31:36.049658Z","shell.execute_reply.started":"2025-11-28T07:31:36.043672Z","shell.execute_reply":"2025-11-28T07:31:36.048272Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# from sklearn.svm import LinearSVC\n\n# def objective_svm(trial):\n#     C = trial.suggest_float(\"C\", 1e-4, 10, log=True)\n#     tol = trial.suggest_float(\"tol\", 1e-5, 1e-1, log=True)\n#     loss = trial.suggest_categorical(\"loss\", [\"hinge\", \"squared_hinge\"])\n\n#     clf = OneVsRestClassifier(\n#         LinearSVC(\n#             C=C,\n#             tol=tol,\n#             loss=loss,\n#             class_weight=\"balanced\",\n#             max_iter=3000\n#         )\n#     )\n\n#     clf.fit(X_train_vec, y_train)\n#     pred = clf.predict(X_test_vec)\n\n#     score = micro_f1(y_test, pred)\n#     return score\n\n# study_svm = optuna.create_study(direction=\"maximize\")\n# study_svm.optimize(objective_svm, n_trials=300)\n\n# print(\"Best SVM Params:\", study_svm.best_params)\n# print(\"Best SVM Score:\", study_svm.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T07:31:39.037793Z","iopub.execute_input":"2025-11-28T07:31:39.038157Z","iopub.status.idle":"2025-11-28T07:31:39.043213Z","shell.execute_reply.started":"2025-11-28T07:31:39.038134Z","shell.execute_reply":"2025-11-28T07:31:39.041980Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import datetime\nimport pickle\n\nbest_lr = OneVsRestClassifier(\n    LogisticRegression(\n        C=study_lr.best_params[\"C\"],\n        solver=study_lr.best_params[\"solver\"],\n        class_weight=\"balanced\",\n        max_iter=2000\n    )\n)\n\nbest_lr, pred_lr = training_ml(\n    best_lr, \n    X_train, \n    X_test, \n    y_train, \n    y_test, \n    use_vectorizer=True\n)\n\ntimestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nlr_filename = f\"best_logreg_{timestamp}.pkl\"\n\nwith open(lr_filename, \"wb\") as f:\n    pickle.dump(best_lr, f)\n\nprint(\"Model Logistic Regression disimpan ke:\", lr_filename)\n\nbest_svm = OneVsRestClassifier(\n    LinearSVC(\n        C=study_svm.best_params[\"C\"],\n        tol=study_svm.best_params[\"tol\"],\n        loss=study_svm.best_params[\"loss\"],\n        class_weight=\"balanced\",\n        max_iter=3000\n    )\n)\n\nbest_svm, pred_svm = training_ml(\n    best_svm, \n    X_train, \n    X_test, \n    y_train, \n    y_test, \n    use_vectorizer=True\n)\n\nsvm_filename = f\"best_svm_{timestamp}.pkl\"\n\nwith open(svm_filename, \"wb\") as f:\n    pickle.dump(best_svm, f)\n\nprint(\"Model LinearSVC disimpan ke:\", svm_filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:16:38.709578Z","iopub.execute_input":"2025-11-26T00:16:38.709891Z","iopub.status.idle":"2025-11-26T00:16:40.117478Z","shell.execute_reply.started":"2025-11-26T00:16:38.709871Z","shell.execute_reply":"2025-11-26T00:16:40.116460Z"}},"outputs":[{"name":"stdout","text":"Model: LogisticRegression\nTraining dimulai...\nTraining selesai.\nDurasi: 0.569 detik\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.67      0.76      0.71       715\n           1       0.57      0.68      0.62       397\n           2       0.53      0.73      0.62       159\n           3       0.58      0.86      0.70       113\n           4       0.56      0.78      0.65        65\n           5       0.49      0.66      0.56        61\n\n   micro avg       0.61      0.74      0.67      1510\n   macro avg       0.57      0.75      0.64      1510\nweighted avg       0.61      0.74      0.67      1510\n samples avg       0.29      0.31      0.29      1510\n\nHamming Loss: 0.07086813464945584\nSubset Accuracy: 0.6761579347000759 \n\nModel Logistic Regression disimpan ke: best_logreg_20251126_001639.pkl\nModel: LinearSVC\nTraining dimulai...\nTraining selesai.\nDurasi: 0.241 detik\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.65      0.76      0.70       715\n           1       0.56      0.70      0.62       397\n           2       0.52      0.77      0.62       159\n           3       0.57      0.88      0.69       113\n           4       0.54      0.82      0.65        65\n           5       0.49      0.66      0.56        61\n\n   micro avg       0.59      0.75      0.66      1510\n   macro avg       0.55      0.76      0.64      1510\nweighted avg       0.60      0.75      0.66      1510\n samples avg       0.29      0.32      0.30      1510\n\nHamming Loss: 0.07314603897747406\nSubset Accuracy: 0.6681852695520122 \n\nModel LinearSVC disimpan ke: best_svm_20251126_001639.pkl\n","output_type":"stream"}],"execution_count":139},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}